{"cells": [{"cell_type": "markdown",
            "metadata": {"id": "40WxNsXm6JOs"},
            "source": ["# Import Libraries"]},
           {"cell_type": "code",
            "execution_count": 65,
            "metadata": {"id": "6lbm0tZpT7JY"},
            "outputs": [],
            "source": ["import pandas as pd"]},
           {"cell_type": "code",
            "execution_count": 66,
            "metadata": {"id": "pj6n824NT8tC"},
            "outputs": [],
            "source": ["import torch\n",
                       "import torch.nn as nn\n",
                       "import pandas as pd\n",
                       "from torch.nn import functional as F\n",
                       "from torch.utils.data import Dataset, DataLoader\n",
                       "from model import GPTLanguageModel  # Import  model class"]},
           {"cell_type": "markdown",
            "metadata": {"id": "dzWqqMSN8mJa"},
            "source": ["# Hyperparameters"]},
           {"cell_type": "code",
            "execution_count": 67,
            "metadata": {"colab": {"base_uri": "https://localhost:8080/"},
                         "id": "GaC35w3yUBu4",
                         "outputId": "f9913e96-e0e7-4133-c92d-cd44c49a3aae"},
            "outputs": [{"data": {"text/plain": ["<torch._C.Generator at 0x1e44e5ccfd0>"]},
                         "execution_count": 67,
                         "metadata": {},
                         "output_type": "execute_result"}],
            "source": ["# hyperparameters\n",
                       "\n",
                       "max_iters = 5000\n",
                       "eval_interval = 100\n",
                       "learning_rate = 1e-3\n",
                       "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                       "eval_iters = 200\n",
                       "batch_size = 16 #  independent sequences processed in parallel\n",
                       "block_size = 32 # maximum context length for predictions\n",
                       "# ------------\n",
                       "\n",
                       "torch.manual_seed(1337)"]},
           {"cell_type": "code",
            "execution_count": 68,
            "metadata": {"id": "lv2v9Gt5UIMi"},
            "outputs": [],
            "source": ["\n",
                       "\n",
                       "\n",
                       "def get_batch(split):\n",
                       "    # generate a small batch of data of inputs x and targets y\n",
                       "    data = train_data if split == 'train' else val_data\n",
                       "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
                       "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
                       "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
                       "    x, y = x.to(device), y.to(device)\n",
                       "    return x, y\n",
                       "\n",
                       "@torch.no_grad()\n",
                       "def estimate_loss():\n",
                       "    out = {}\n",
                       "    model.eval()\n",
                       "    for split in ['train', 'val']:\n",
                       "        losses = torch.zeros(eval_iters)\n",
                       "        for k in range(eval_iters):\n",
                       "            X, Y = get_batch(split)\n",
                       "            logits, loss = model(X, Y)\n",
                       "            losses[k] = loss.item()\n",
                       "        out[split] = losses.mean()\n",
                       "    model.train()\n",
                       "    return out\n"]},
           {"cell_type": "markdown",
            "metadata": {"id": "0HtUlhuR8rxX"},
            "source": ["# Loading the Dataset"]},
           {"cell_type": "code",
            "execution_count": 69,
            "metadata": {"id": "z5kEcCVNUfgL"},
            "outputs": [],
            "source": ["\n",
                       "with open('binary_operation_fine_shuffled_file.csv', 'r', encoding='utf-8') as f:\n",
                       "    text = f.read()"]},
           {"cell_type": "code",
            "execution_count": 70,
            "metadata": {"id": "QLGOB5YBU4CB"},
            "outputs": [],
            "source": ["chars = sorted(list(set(text)))\n",
                       "vocab_size = len(chars)\n",
                       "# create a mapping from characters to integers\n",
                       "stoi = { ch:i for i,ch in enumerate(chars) }\n",
                       "itos = { i:ch for i,ch in enumerate(chars) }\n",
                       "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
                       "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"]},
           {"cell_type": "code",
            "execution_count": 71,
            "metadata": {"id": "oNrLmkYCV_gn"},
            "outputs": [],
            "source": ["# Train and test splits\n",
                       "data = torch.tensor(encode(text), dtype=torch.long)\n",
                       "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
                       "train_data = data[:n]\n",
                       "val_data = data[n:]\n",
                       "\n"]},
           {"cell_type": "code",
            "execution_count": null,
            "metadata": {"colab": {"base_uri": "https://localhost:8080/"},
                         "id": "wEPb1_VrVoQx",
                         "outputId": "a6752b92-8fdc-48c5-b5f2-f30abaaff871"},
            "outputs": [],
            "source": ["model = GPTLanguageModel()\n",
                       "m = model.to(device)\n",
                       "# print the number of parameters in the model\n",
                       "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
                       "\n",
                       "# create a PyTorch optimizer\n",
                       "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"]},
           {"cell_type": "code",
            "execution_count": 55,
            "metadata": {"id": "gwPPkVsFXOYH"},
            "outputs": [],
            "source": ["from torch.nn.utils.rnn import pad_sequence"]},
           {"cell_type": "markdown",
            "metadata": {"id": "Drr0wxzq812o"},
            "source": ["# Train the model"]},
           {"cell_type": "code",
            "execution_count": 11,
            "metadata": {"colab": {"base_uri": "https://localhost:8080/"},
                         "id": "aNzgNWGFVw4x",
                         "outputId": "fcdc65a1-dcfd-4641-c6bd-3f1127451b46"},
            "outputs": [{"name": "stdout",
                         "output_type": "stream",
                         "text": ["step 0: train loss 3.8200, val loss 3.8202\n",
                                  "step 100: train loss 1.5247, val loss 1.5334\n",
                                  "step 200: train loss 1.1749, val loss 1.1842\n",
                                  "step 300: train loss 1.1052, val loss 1.1258\n",
                                  "step 400: train loss 1.0577, val loss 1.0606\n",
                                  "step 500: train loss 1.0185, val loss 1.0185\n",
                                  "step 600: train loss 0.9376, val loss 0.9546\n",
                                  "step 700: train loss 0.8900, val loss 0.9062\n",
                                  "step 800: train loss 0.8407, val loss 0.8733\n",
                                  "step 900: train loss 0.8114, val loss 0.8278\n",
                                  "step 1000: train loss 0.8061, val loss 0.8199\n",
                                  "step 1100: train loss 0.7769, val loss 0.7954\n",
                                  "step 1200: train loss 0.7608, val loss 0.7755\n",
                                  "step 1300: train loss 0.7429, val loss 0.7706\n",
                                  "step 1400: train loss 0.7482, val loss 0.7652\n",
                                  "step 1500: train loss 0.7372, val loss 0.7589\n",
                                  "step 1600: train loss 0.7238, val loss 0.7496\n",
                                  "step 1700: train loss 0.7230, val loss 0.7397\n",
                                  "step 1800: train loss 0.7247, val loss 0.7454\n",
                                  "step 1900: train loss 0.7001, val loss 0.7264\n",
                                  "step 2000: train loss 0.7054, val loss 0.7194\n",
                                  "step 2100: train loss 0.6932, val loss 0.7101\n",
                                  "step 2200: train loss 0.6853, val loss 0.7026\n",
                                  "step 2300: train loss 0.6840, val loss 0.7058\n",
                                  "step 2400: train loss 0.6750, val loss 0.6920\n",
                                  "step 2500: train loss 0.6762, val loss 0.6898\n",
                                  "step 2600: train loss 0.6702, val loss 0.6914\n",
                                  "step 2700: train loss 0.6732, val loss 0.6887\n",
                                  "step 2800: train loss 0.6698, val loss 0.6862\n",
                                  "step 2900: train loss 0.6733, val loss 0.6831\n",
                                  "step 3000: train loss 0.6591, val loss 0.6776\n",
                                  "step 3100: train loss 0.6564, val loss 0.6714\n",
                                  "step 3200: train loss 0.6520, val loss 0.6676\n",
                                  "step 3300: train loss 0.6656, val loss 0.6791\n",
                                  "step 3400: train loss 0.6590, val loss 0.6660\n",
                                  "step 3500: train loss 0.6509, val loss 0.6615\n",
                                  "step 3600: train loss 0.6483, val loss 0.6634\n",
                                  "step 3700: train loss 0.6440, val loss 0.6567\n",
                                  "step 3800: train loss 0.6481, val loss 0.6614\n",
                                  "step 3900: train loss 0.6423, val loss 0.6557\n",
                                  "step 4000: train loss 0.6377, val loss 0.6539\n",
                                  "step 4100: train loss 0.6394, val loss 0.6545\n",
                                  "step 4200: train loss 0.6396, val loss 0.6498\n",
                                  "step 4300: train loss 0.6412, val loss 0.6507\n",
                                  "step 4400: train loss 0.6343, val loss 0.6447\n",
                                  "step 4500: train loss 0.6290, val loss 0.6423\n",
                                  "step 4600: train loss 0.6360, val loss 0.6429\n",
                                  "step 4700: train loss 0.6277, val loss 0.6401\n",
                                  "step 4800: train loss 0.6282, val loss 0.6409\n",
                                  "step 4900: train loss 0.6258, val loss 0.6410\n",
                                  "step 4999: train loss 0.6253, val loss 0.6369\n"]}],
            "source": ["for iter in range(max_iters):\n",
                       "\n",
                       "    # every once in a while evaluate the loss on train and val sets\n",
                       "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
                       "        losses = estimate_loss()\n",
                       "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
                       "\n",
                       "    # sample a batch of data\n",
                       "    xb, yb = get_batch('train')\n",
                       "\n",
                       "    # evaluate the loss\n",
                       "    logits, loss = model(xb, yb)\n",
                       "    optimizer.zero_grad(set_to_none=True)\n",
                       "    loss.backward()\n",
                       "    optimizer.step()"]},
           {"cell_type": "code",
            "execution_count": 12,
            "metadata": {"id": "TEHwUrKnYP_X"},
            "outputs": [],
            "source": ["import re\n",
                       "\n",
                       "def generate_prompt_response(prompt, model, max_new_tokens):\n",
                       "    model.eval()\n",
                       "    input_tokens = encode(prompt)\n",
                       "    context = torch.tensor(input_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
                       "    generated_tokens = model.generate(context, max_new_tokens=max_new_tokens)[0].tolist()\n",
                       "    generated_response = decode(generated_tokens)\n",
                       "\n",
                       "    # Use regular expression to match the answer format (e.g., \"3+4=7\"or \"3x4=12\")\n",
                       "    match = re.search(r'\\d+\\s*([-+\\/xX])\\s*\\d+\\s*=\\s*\\d+', generated_response)\n",
                       "    if match:\n",
                       "        answer = match.group(0)\n",
                       "        return answer\n",
                       "\n",
                       "    return \"Answer not found\"\n",
                       "\n",
                       "trained_model = m\n"]},
           {"cell_type": "code",
            "execution_count": 31,
            "metadata": {"colab": {"base_uri": "https://localhost:8080/"},
                         "id": "1BW6QtD4YWOw",
                         "outputId": "684658af-889d-4602-b1b4-2947f524b1b5"},
            "outputs": [{"name": "stdout",
                         "output_type": "stream",
                         "text": ["Input Prompt: hey, what's 3+13?\n",
                                  "Generated Response: 3+13=15\n"]}],
            "source": ["prompt = \"hey, what's 3+13?\"  # The expected result is 16\n",
                       "max_new_tokens = 15  # Maximum number of tokens in the generated response\n",
                       "\n",
                       "# Generate response based on the prompt\n",
                       "generated_response = generate_prompt_response(prompt, trained_model, max_new_tokens)\n",
                       "print(f\"Input Prompt: {prompt}\\nGenerated Response: {generated_response}\")"]},
           {"cell_type": "code",
            "execution_count": 14,
            "metadata": {"colab": {"base_uri": "https://localhost:8080/"},
                         "id": "9u_QbNr6eV1y",
                         "outputId": "30a5edcb-9a7a-4a7f-e874-855848a7a4ce"},
            "outputs": [{"name": "stdout",
                         "output_type": "stream",
                         "text": ["Input Prompt: What is 12x10?\n",
                                  "Generated Response: 12x10=140\n"]}],
            "source": ["prompt = \"What is 12x10?\"  # The expected result is 120\n",
                       "max_new_tokens = 15  # Maximum number of tokens in the generated response\n",
                       "\n",
                       "# Generate response based on the prompt\n",
                       "generated_response = generate_prompt_response(prompt, trained_model, max_new_tokens)\n",
                       "print(f\"Input Prompt: {prompt}\\nGenerated Response: {generated_response}\")"]},
           {"cell_type": "code",
            "execution_count": 25,
            "metadata": {"colab": {"base_uri": "https://localhost:8080/"},
                         "id": "7sYHL7Gn05DR",
                         "outputId": "0fe451ff-27bb-4784-c937-0b80200524b8"},
            "outputs": [{"name": "stdout",
                         "output_type": "stream",
                         "text": ["Input Prompt: What is 12/6?\n",
                                  "Generated Response: 12/6=2\n"]}],
            "source": ["prompt = \"What is 12/6?\"  # The expected result is 2\n",
                       "max_new_tokens = 15  # Maximum number of tokens in the generated response\n",
                       "\n",
                       "# Generate response based on the prompt\n",
                       "generated_response = generate_prompt_response(prompt, trained_model, max_new_tokens)\n",
                       "print(f\"Input Prompt: {prompt}\\nGenerated Response: {generated_response}\")"]},
           {"cell_type": "code",
            "execution_count": 26,
            "metadata": {"colab": {"base_uri": "https://localhost:8080/"},
                         "id": "6_vFq2Hb1Cgk",
                         "outputId": "f5dc9739-d163-4514-bd44-28dc5a1ee4ef"},
            "outputs": [{"name": "stdout",
                         "output_type": "stream",
                         "text": ["Input Prompt: value of 15+5?\n",
                                  "Generated Response: 15+5=22\n"]}],
            "source": ["prompt = \"value of 15+5?\"  # The expected result is 20\n",
                       "max_new_tokens = 15  # Maximum number of tokens in the generated response\n",
                       "\n",
                       "# Generate response based on the prompt\n",
                       "generated_response = generate_prompt_response(prompt, trained_model, max_new_tokens)\n",
                       "print(f\"Input Prompt: {prompt}\\nGenerated Response: {generated_response}\")"]}],
    "metadata": {"accelerator": "GPU",
                 "colab": {"gpuType": "T4",
                           "provenance": []},
                 "kernelspec": {"display_name": "Python 3",
                                "name": "python3"},
                 "language_info": {"codemirror_mode": {"name": "ipython",
                                                       "version": 3},
                                   "file_extension": ".py",
                                   "mimetype": "text/x-python",
                                   "name": "python",
                                   "nbconvert_exporter": "python",
                                   "pygments_lexer": "ipython3",
                                   "version": "3.8.8"}},
    "nbformat": 4,
 "nbformat_minor": 0}
